{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing core directory\n",
    "import os, sys\n",
    "dir2 = os.path.abspath('')\n",
    "dir1 = os.path.dirname(dir2)\n",
    "if not dir1 in sys.path:\n",
    "    sys.path.append(dir1)\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import warnings\n",
    "import yaml\n",
    "\n",
    "from replay_benchmarks.utils.conf import load_config, seed_everything\n",
    "from replay_benchmarks import TrainRunner, InferRunner\n",
    "\n",
    "import logging\n",
    "import os\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Tuple\n",
    "\n",
    "import pandas as pd\n",
    "from rs_datasets import MovieLens, Netflix\n",
    "\n",
    "from replay.data import (\n",
    "    FeatureHint,\n",
    "    FeatureInfo,\n",
    "    FeatureSchema,\n",
    "    FeatureSource,\n",
    "    FeatureType,\n",
    "    Dataset,\n",
    ")\n",
    "from replay.preprocessing.filters import MinCountFilter, NumInteractionsFilter\n",
    "from replay.splitters import TimeSplitter\n",
    "from replay.utils import DataFrameLike\n",
    "from replay.data.nn import (\n",
    "    SequenceTokenizer,\n",
    "    SequentialDataset,\n",
    "    TensorFeatureSource,\n",
    "    TensorSchema,\n",
    "    TensorFeatureInfo,\n",
    ")\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from replay.models.nn.sequential.sasrec import (\n",
    "    SasRecTrainingDataset,\n",
    "    SasRecValidationDataset,\n",
    "    SasRecPredictionDataset,\n",
    ")\n",
    "from replay.models.nn.sequential.bert4rec import (\n",
    "    Bert4RecTrainingDataset,\n",
    "    Bert4RecValidationDataset,\n",
    "    Bert4RecPredictionDataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dir = \"./replay_benchmarks/configs\"\n",
    "base_config_path = os.path.join(config_dir, \"config.yaml\")\n",
    "config = load_config(base_config_path, config_dir)\n",
    "\n",
    "seed_everything(config[\"env\"][\"SEED\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = config\n",
    "model_name = config[\"model\"][\"name\"]\n",
    "dataset_name = config[\"dataset\"][\"name\"]\n",
    "dataset_cfg = config[\"dataset\"]\n",
    "model_cfg = config[\"model\"][\"params\"]\n",
    "mode = config[\"mode\"][\"name\"]\n",
    "item_column = dataset_cfg[\"feature_schema\"][\"item_column\"]\n",
    "user_column = dataset_cfg[\"feature_schema\"][\"query_column\"]\n",
    "timestamp_column = dataset_cfg[\"feature_schema\"][\"timestamp_column\"]\n",
    "tokenizer = None\n",
    "interactions = None\n",
    "user_features = None\n",
    "item_features = None\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = config[\"env\"][\"CUDA_DEVICE_ORDER\"]\n",
    "os.environ[\"OMP_NUM_THREADS\"] = config[\"env\"][\"OMP_NUM_THREADS\"]\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = config[\"env\"][\"CUDA_VISIBLE_DEVICES\"]\n",
    "os.environ[\"KAGGLE_USERNAME\"] = \"recsysaccelerate\"\n",
    "os.environ[\"KAGGLE_KEY\"] = \"6363e91b656fea576c39e4f55dcc1d00\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<replay.data.nn.schema.TensorSchema at 0x7f3ec32f0a50>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_dim = model_cfg[\"training_params\"][\"embedding_dim\"]\n",
    "item_feature_name = \"item_id_seq\"\n",
    "\n",
    "tensor_schema =  TensorSchema(\n",
    "    TensorFeatureInfo(\n",
    "        name=item_feature_name,\n",
    "        is_seq=True,\n",
    "        feature_type=FeatureType.CATEGORICAL,\n",
    "        feature_sources=[\n",
    "            TensorFeatureSource(\n",
    "                FeatureSource.INTERACTIONS,\n",
    "                item_column,\n",
    "            )\n",
    "        ],\n",
    "        feature_hint=FeatureHint.ITEM_ID,\n",
    "        embedding_dim=embedding_dim,\n",
    "    )\n",
    ")\n",
    "\n",
    "tensor_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_MAPPINGS = {\n",
    "    \"zvuk\": {\"kaggle\": \"alexxl/zvuk-dataset\", \"file\": \"zvuk-interactions.parquet\"},\n",
    "    \"megamarket\": {\"kaggle\": \"alexxl/megamarket\", \"file\": \"megamarket.parquet\"},\n",
    "}\n",
    "SUPPORTED_RS_DATASETS = [\"movielens\", \"netflix\"]\n",
    "\n",
    "def _download_dataset(\n",
    "    data_path: str, dataset_name: str, interactions_file: str\n",
    "):\n",
    "    \"\"\"Download dataset from Kaggle or rs_datasets.\"\"\"\n",
    "    if dataset_name in DATASET_MAPPINGS:\n",
    "        _download_kaggle_dataset(data_path, dataset_name, interactions_file)\n",
    "    elif any(ds in dataset_name for ds in SUPPORTED_RS_DATASETS):\n",
    "        _download_rs_dataset(data_path, dataset_name, interactions_file)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported dataset: {dataset_name}\")\n",
    "\n",
    "def _download_kaggle_dataset(\n",
    "    data_path: str, dataset_name: str, interactions_file: str\n",
    ") -> None:\n",
    "    from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "\n",
    "    \"\"\"Download dataset from Kaggle.\"\"\"\n",
    "    kaggle_info = DATASET_MAPPINGS[dataset_name]\n",
    "    kaggle_dataset = kaggle_info[\"kaggle\"]\n",
    "    raw_data_file = os.path.join(data_path, kaggle_info[\"file\"])\n",
    "\n",
    "    os.environ.setdefault(\"KAGGLE_USERNAME\", \"recsysaccelerate\")\n",
    "    os.environ.setdefault(\"KAGGLE_KEY\", \"6363e91b656fea576c39e4f55dcc1d00\")\n",
    "\n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "\n",
    "    api.dataset_download_files(kaggle_dataset, path=data_path, unzip=True)\n",
    "    logging.info(f\"Dataset downloaded and extracted to {data_path}\")\n",
    "\n",
    "    interactions = pd.read_parquet(raw_data_file)\n",
    "    interactions[timestamp_column] = interactions[\n",
    "        timestamp_column\n",
    "    ].astype(\"int64\")\n",
    "    if dataset_name == \"megamarket\":\n",
    "        interactions = interactions[interactions.event == 2] # take only purchase\n",
    "    interactions.to_parquet(interactions_file)\n",
    "\n",
    "def _download_rs_dataset(\n",
    "    data_path: str, dataset_name: str, interactions_file: str\n",
    ") -> None:\n",
    "    \"\"\"Download dataset from rs_datasets.\"\"\"\n",
    "    if \"movielens\" in dataset_name:\n",
    "        version = dataset_name.split(\"_\")[1]\n",
    "        movielens = MovieLens(version=version, path=data_path)\n",
    "        interactions = movielens.ratings\n",
    "        interactions = interactions[interactions[dataset_cfg[\"feature_schema\"][\"rating_column\"]] > dataset_cfg[\"preprocess\"][\"min_rating\"]]\n",
    "    elif dataset_name == \"netflix\":\n",
    "        netflix = Netflix(path=data_path)\n",
    "        interactions = pd.concat([netflix.train, netflix.test]).fillna(5).reset_index(drop=True)\n",
    "        interactions = interactions[interactions[dataset_cfg[\"feature_schema\"][\"rating_column\"]] > dataset_cfg[\"preprocess\"][\"min_rating\"]]\n",
    "        interactions = interactions.sort_values(by=[user_column, timestamp_column])\n",
    "        interactions[timestamp_column] += interactions.groupby([user_column, timestamp_column]).cumcount()\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported dataset: {dataset_name}\")\n",
    "\n",
    "    interactions[timestamp_column] = interactions[\n",
    "        timestamp_column\n",
    "    ].astype(\"int64\")\n",
    "    interactions.to_parquet(interactions_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>978300760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "      <td>978300275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "      <td>978824291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>1287</td>\n",
       "      <td>5</td>\n",
       "      <td>978302039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>2804</td>\n",
       "      <td>5</td>\n",
       "      <td>978300719</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating  timestamp\n",
       "0        1     1193       5  978300760\n",
       "3        1     3408       4  978300275\n",
       "4        1     2355       5  978824291\n",
       "6        1     1287       5  978302039\n",
       "7        1     2804       5  978300719"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_name = dataset_cfg[\"name\"]\n",
    "data_path = dataset_cfg[\"path\"]\n",
    "interactions_file = os.path.join(data_path, \"interactions.parquet\")\n",
    "\n",
    "if not os.path.exists(interactions_file):\n",
    "    _download_dataset(data_path, dataset_name, interactions_file)\n",
    "\n",
    "interactions = pd.read_parquet(interactions_file)\n",
    "interactions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4, 5)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(interactions.groupby(user_column).size().min(), interactions.groupby(item_column).size().min())\n",
    "\n",
    "interactions = MinCountFilter(\n",
    "    num_entries=dataset_cfg[\"preprocess\"][\"min_users_per_item\"],\n",
    "    groupby_column=item_column,\n",
    ").transform(interactions)\n",
    "\n",
    "interactions = MinCountFilter(\n",
    "    num_entries=dataset_cfg[\"preprocess\"][\"min_items_per_user\"],\n",
    "    groupby_column=user_column,\n",
    ").transform(interactions)\n",
    "\n",
    "\n",
    "interactions.groupby(user_column).size().min(), interactions.groupby(item_column).size().min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_cfg[\"preprocess\"][\"global_split_ratio\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'user_id'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution of seq_len in validation:\n",
      "count    448.000000\n",
      "mean      24.837054\n",
      "std       49.389768\n",
      "min        1.000000\n",
      "25%        2.000000\n",
      "50%        6.500000\n",
      "75%       26.000000\n",
      "max      656.000000\n",
      "Name: item_id, dtype: float64.\n",
      "Distribution of seq_len in test:\n",
      "count    999.000000\n",
      "mean      43.796797\n",
      "std       59.198925\n",
      "min        1.000000\n",
      "25%        7.000000\n",
      "50%       23.000000\n",
      "75%       57.000000\n",
      "max      585.000000\n",
      "Name: item_id, dtype: float64.\n"
     ]
    }
   ],
   "source": [
    "splitter = TimeSplitter(\n",
    "    time_threshold=dataset_cfg[\"preprocess\"][\"global_split_ratio\"],\n",
    "    drop_cold_users=True,\n",
    "    drop_cold_items=True,\n",
    "    item_column=item_column,\n",
    "    query_column=user_column,\n",
    "    timestamp_column=timestamp_column,\n",
    ")\n",
    "\n",
    "# train_events, validation_events, validation_gt, test_events, test_gt = (\n",
    "#     _split_data(splitter, interactions)\n",
    "# )\n",
    "\n",
    "test_events, test_gt = splitter.split(interactions)\n",
    "validation_events, validation_gt = splitter.split(test_events)\n",
    "train_events = validation_events\n",
    "\n",
    "test_gt = test_gt[test_gt[item_column].isin(train_events[item_column])]\n",
    "test_gt = test_gt[test_gt[user_column].isin(train_events[user_column])]\n",
    "\n",
    "\n",
    "# Limit number of gt events in val and test only if max_num_test_interactions is not null\n",
    "max_test_interactions = dataset_cfg[\"preprocess\"][\"max_num_test_interactions\"]\n",
    "print(f\"Distribution of seq_len in validation:\\n{validation_gt.groupby(user_column)[item_column].agg('count').describe()}.\")\n",
    "print(f\"Distribution of seq_len in test:\\n{test_gt.groupby(user_column)[item_column].agg('count').describe()}.\")\n",
    "if max_test_interactions is not None:\n",
    "    \n",
    "    validation_gt = NumInteractionsFilter(\n",
    "        num_interactions=max_test_interactions,\n",
    "        first=True,\n",
    "        query_column=user_column,\n",
    "        item_column=item_column,\n",
    "        timestamp_column=timestamp_column,\n",
    "    ).transform(validation_gt)\n",
    "    print(f\"Distribution of seq_len in validation  after filtering:\\n{validation_gt.groupby(user_column)[item_column].agg('count').describe()}.\")\n",
    "\n",
    "    test_gt = NumInteractionsFilter(\n",
    "        num_interactions=max_test_interactions,\n",
    "        first=True,\n",
    "        query_column=user_column,\n",
    "        item_column=item_column,\n",
    "        timestamp_column=timestamp_column,\n",
    "    ).transform(test_gt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>140725</th>\n",
       "      <td>904</td>\n",
       "      <td>2348</td>\n",
       "      <td>4</td>\n",
       "      <td>977939424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140721</th>\n",
       "      <td>904</td>\n",
       "      <td>858</td>\n",
       "      <td>4</td>\n",
       "      <td>977939543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140660</th>\n",
       "      <td>904</td>\n",
       "      <td>260</td>\n",
       "      <td>5</td>\n",
       "      <td>977939543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140668</th>\n",
       "      <td>904</td>\n",
       "      <td>2866</td>\n",
       "      <td>5</td>\n",
       "      <td>977939573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140696</th>\n",
       "      <td>904</td>\n",
       "      <td>1193</td>\n",
       "      <td>4</td>\n",
       "      <td>977939573</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id  item_id  rating  timestamp\n",
       "140725      904     2348       4  977939424\n",
       "140721      904      858       4  977939543\n",
       "140660      904      260       5  977939543\n",
       "140668      904     2866       5  977939573\n",
       "140696      904     1193       4  977939573"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_gt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_events['timestamp'].max() <= validation_events['timestamp'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('timestamp', 'item_id', 'user_id')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timestamp_column, item_column, user_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_splitting(events, gt, name=''):\n",
    "    if events[timestamp_column].max() > gt[timestamp_column].min():\n",
    "        print(\"Problem with time points in\", name)\n",
    "    if len(set(gt[user_column].unique().tolist()) - set(events[user_column].unique().tolist())) > 0:\n",
    "        print(\"Problem with cold users in\", name)\n",
    "    if len(set(gt[item_column].unique().tolist()) - set(events[item_column].unique().tolist())) > 0:\n",
    "        print(\"Problem with cold items in\", name)\n",
    "\n",
    "\n",
    "test_splitting(train_events, test_gt, \"train events, test gt\")\n",
    "test_splitting(train_events, validation_gt, \"train events, valid gt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([1, 2, 3])  - set({1, 2, 3, 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_feature_schema(is_ground_truth: bool) -> FeatureSchema:\n",
    "    \"\"\"Prepare the feature schema based on whether ground truth is needed.\"\"\"\n",
    "    base_features = FeatureSchema(\n",
    "        [\n",
    "            FeatureInfo(\n",
    "                column=user_column,\n",
    "                feature_hint=FeatureHint.QUERY_ID,\n",
    "                feature_type=FeatureType.CATEGORICAL,\n",
    "            ),\n",
    "            FeatureInfo(\n",
    "                column=item_column,\n",
    "                feature_hint=FeatureHint.ITEM_ID,\n",
    "                feature_type=FeatureType.CATEGORICAL,\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    if is_ground_truth:\n",
    "        return base_features\n",
    "\n",
    "    return base_features + FeatureSchema(\n",
    "        [\n",
    "            FeatureInfo(\n",
    "                column=timestamp_column,\n",
    "                feature_type=FeatureType.NUMERICAL,\n",
    "                feature_hint=FeatureHint.TIMESTAMP,\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "feature_schema = prepare_feature_schema(is_ground_truth=False)\n",
    "ground_truth_schema = prepare_feature_schema(is_ground_truth=True)\n",
    "\n",
    "train_dataset = Dataset(\n",
    "    feature_schema=feature_schema,\n",
    "    interactions=train_events,\n",
    "    query_features=user_features,\n",
    "    item_features=item_features,\n",
    "    check_consistency=True,\n",
    "    categorical_encoded=False,\n",
    ")\n",
    "validation_dataset = Dataset(\n",
    "    feature_schema=feature_schema,\n",
    "    interactions=validation_events,\n",
    "    query_features=user_features,\n",
    "    item_features=item_features,\n",
    "    check_consistency=True,\n",
    "    categorical_encoded=False,\n",
    ")\n",
    "validation_gt_dataset = Dataset(\n",
    "    feature_schema=ground_truth_schema,\n",
    "    interactions=validation_gt,\n",
    "    check_consistency=True,\n",
    "    categorical_encoded=False,\n",
    ")\n",
    "test_dataset = Dataset(\n",
    "    feature_schema=feature_schema,\n",
    "    interactions=test_events,\n",
    "    query_features=user_features,\n",
    "    item_features=item_features,\n",
    "    check_consistency=True,\n",
    "    categorical_encoded=False,\n",
    ")\n",
    "test_gt_dataset = Dataset(\n",
    "    feature_schema=ground_truth_schema,\n",
    "    interactions=test_gt,\n",
    "    check_consistency=True,\n",
    "    categorical_encoded=False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/RePlay-Accelerated/replay/data/nn/sequence_tokenizer.py:396: UserWarning: The specified cardinality of item_id_seq will be replaced by item_id from Dataset\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SequenceTokenizer(\n",
    "    tensor_schema, allow_collect_to_master=True, handle_unknown_rule=\"drop\"\n",
    ")\n",
    "tokenizer.fit(train_dataset)\n",
    "\n",
    "seq_train_dataset = tokenizer.transform(train_dataset)\n",
    "# seq_validation_dataset, seq_validation_gt = _prepare_sequential_validation(\n",
    "#     validation_dataset, validation_gt\n",
    "# )\n",
    "\n",
    "seq_validation_dataset = tokenizer.transform(validation_dataset)\n",
    "seq_validation_gt = tokenizer.transform(\n",
    "    validation_gt_dataset, [tensor_schema.item_id_feature_name]\n",
    ")\n",
    "\n",
    "seq_validation_dataset, seq_validation_gt = SequentialDataset.keep_common_query_ids(\n",
    "            seq_validation_dataset, seq_validation_gt\n",
    ")\n",
    "\n",
    "\n",
    "test_query_ids = test_gt_dataset.query_ids\n",
    "test_query_ids_np = tokenizer.query_id_encoder.transform(test_query_ids)[\n",
    "    user_column\n",
    "].values\n",
    "seq_test_dataset = tokenizer.transform(test_dataset).filter_by_query_id(\n",
    "    test_query_ids_np\n",
    ")\n",
    "# seq_test_dataset = self._prepare_sequential_test(test_dataset, test_gt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_mapping = {\n",
    "    \"sasrec\": (\n",
    "        SasRecTrainingDataset,\n",
    "        SasRecValidationDataset,\n",
    "        SasRecPredictionDataset,\n",
    "    ),\n",
    "    \"bert4rec\": (\n",
    "        Bert4RecTrainingDataset,\n",
    "        Bert4RecValidationDataset,\n",
    "        Bert4RecPredictionDataset,\n",
    "    ),\n",
    "}\n",
    "\n",
    "if model_name.lower() in dataset_mapping:\n",
    "    TrainingDataset, ValidationDataset, PredictionDataset = dataset_mapping[\n",
    "        model_name.lower()\n",
    "    ]\n",
    "else:\n",
    "    raise ValueError(\n",
    "        f\"Unsupported model type for dataloaders: {model_name}\"\n",
    "    )\n",
    "\n",
    "common_params = {\n",
    "    \"batch_size\": model_cfg[\"training_params\"][\"batch_size\"],\n",
    "    \"num_workers\": model_cfg[\"training_params\"][\"num_workers\"],\n",
    "    \"pin_memory\": True,\n",
    "}\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=TrainingDataset(\n",
    "        seq_train_dataset,\n",
    "        max_sequence_length=model_cfg[\"model_params\"][\"max_seq_len\"],\n",
    "    ),\n",
    "    shuffle=True,\n",
    "    **common_params,\n",
    ")\n",
    "val_dataloader = DataLoader(\n",
    "    dataset=ValidationDataset(\n",
    "        seq_validation_dataset,\n",
    "        seq_validation_gt,\n",
    "        seq_train_dataset,\n",
    "        max_sequence_length=model_cfg[\"model_params\"][\"max_seq_len\"],\n",
    "    ),\n",
    "    **common_params,\n",
    ")\n",
    "prediction_dataloader = DataLoader(\n",
    "    dataset=PredictionDataset(\n",
    "        seq_test_dataset,\n",
    "        max_sequence_length=model_cfg[\"model_params\"][\"max_seq_len\"],\n",
    "    ),\n",
    "    **common_params,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
