{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of the SASRec training and inference stages\n",
    "Note that all the given examples can be run without using PySpark, using only Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "from replay.metrics import OfflineMetrics, Recall, Precision, MAP, NDCG, HitRate, MRR\n",
    "from replay.metrics.torch_metrics_builder import metrics_to_df\n",
    "from replay.splitters import LastNSplitter\n",
    "from replay.data import (\n",
    "    FeatureHint,\n",
    "    FeatureInfo,\n",
    "    FeatureSchema,\n",
    "    FeatureSource,\n",
    "    FeatureType,\n",
    "    Dataset,\n",
    ")\n",
    "from replay.models.nn.optimizer_utils import FatOptimizerFactory\n",
    "from replay.models.nn.sequential.callbacks import (\n",
    "    ValidationMetricsCallback,\n",
    "    SparkPredictionCallback,\n",
    "    PandasPredictionCallback, \n",
    "    TorchPredictionCallback,\n",
    "    QueryEmbeddingsPredictionCallback\n",
    ")\n",
    "from replay.models.nn.sequential.postprocessors import RemoveSeenItems\n",
    "from replay.data.nn import (\n",
    "    SequenceTokenizer,\n",
    "    SequentialDataset,\n",
    "    TensorFeatureSource,\n",
    "    TensorSchema,\n",
    "    TensorFeatureInfo\n",
    ")\n",
    "from replay.models.nn.sequential import SasRec\n",
    "from replay.models.nn.sequential.sasrec import (\n",
    "    SasRecPredictionDataset,\n",
    "    SasRecTrainingDataset,\n",
    "    SasRecValidationDataset,\n",
    "    SasRecPredictionBatch,\n",
    "    SasRecModel\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data\n",
    "### Load raw movielens-1M interactions, item features and user features.\n",
    "In the current implementation, the SASRec does not take into account the features of items or users. They are only used to get a complete list of users and items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rs-datasets in /root/miniconda3/envs/prp39/lib/python3.9/site-packages (0.5.1)\n",
      "Requirement already satisfied: datatable in /root/miniconda3/envs/prp39/lib/python3.9/site-packages (from rs-datasets) (1.1.0)\n",
      "Requirement already satisfied: pandas in /root/miniconda3/envs/prp39/lib/python3.9/site-packages (from rs-datasets) (2.0.3)\n",
      "Requirement already satisfied: gdown in /root/miniconda3/envs/prp39/lib/python3.9/site-packages (from rs-datasets) (5.2.0)\n",
      "Requirement already satisfied: pyarrow in /root/miniconda3/envs/prp39/lib/python3.9/site-packages (from rs-datasets) (16.0.0)\n",
      "Requirement already satisfied: tqdm in /root/miniconda3/envs/prp39/lib/python3.9/site-packages (from rs-datasets) (4.66.4)\n",
      "Requirement already satisfied: xlrd in /root/miniconda3/envs/prp39/lib/python3.9/site-packages (from rs-datasets) (2.0.1)\n",
      "Requirement already satisfied: kaggle in /root/miniconda3/envs/prp39/lib/python3.9/site-packages (from rs-datasets) (1.6.17)\n",
      "Requirement already satisfied: py7zr in /root/miniconda3/envs/prp39/lib/python3.9/site-packages (from rs-datasets) (0.22.0)\n",
      "Requirement already satisfied: openpyxl in /root/miniconda3/envs/prp39/lib/python3.9/site-packages (from rs-datasets) (3.1.5)\n",
      "Requirement already satisfied: beautifulsoup4 in /root/miniconda3/envs/prp39/lib/python3.9/site-packages (from gdown->rs-datasets) (4.12.3)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/envs/prp39/lib/python3.9/site-packages (from gdown->rs-datasets) (3.14.0)\n",
      "Requirement already satisfied: requests[socks] in /root/miniconda3/envs/prp39/lib/python3.9/site-packages (from gdown->rs-datasets) (2.32.3)\n",
      "Requirement already satisfied: six>=1.10 in /root/miniconda3/envs/prp39/lib/python3.9/site-packages (from kaggle->rs-datasets) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2023.7.22 in /root/miniconda3/envs/prp39/lib/python3.9/site-packages (from kaggle->rs-datasets) (2024.6.2)\n",
      "Requirement already satisfied: python-dateutil in /root/miniconda3/envs/prp39/lib/python3.9/site-packages (from kaggle->rs-datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: python-slugify in /root/miniconda3/envs/prp39/lib/python3.9/site-packages (from kaggle->rs-datasets) (8.0.4)\n",
      "Requirement already satisfied: urllib3 in /root/miniconda3/envs/prp39/lib/python3.9/site-packages (from kaggle->rs-datasets) (2.2.1)\n",
      "Requirement already satisfied: bleach in /root/miniconda3/envs/prp39/lib/python3.9/site-packages (from kaggle->rs-datasets) (6.1.0)\n",
      "Requirement already satisfied: et-xmlfile in /root/miniconda3/envs/prp39/lib/python3.9/site-packages (from openpyxl->rs-datasets) (1.1.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/envs/prp39/lib/python3.9/site-packages (from pandas->rs-datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /root/miniconda3/envs/prp39/lib/python3.9/site-packages (from pandas->rs-datasets) (2024.1)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /root/miniconda3/envs/prp39/lib/python3.9/site-packages (from pandas->rs-datasets) (1.24.4)\n",
      "Requirement already satisfied: texttable in /root/miniconda3/envs/prp39/lib/python3.9/site-packages (from py7zr->rs-datasets) (1.7.0)\n",
      "Requirement already satisfied: pycryptodomex>=3.16.0 in /root/miniconda3/envs/prp39/lib/python3.9/site-packages (from py7zr->rs-datasets) (3.21.0)\n",
      "Requirement already satisfied: pyzstd>=0.15.9 in /root/miniconda3/envs/prp39/lib/python3.9/site-packages (from py7zr->rs-datasets) (0.16.2)\n",
      "Requirement already satisfied: pyppmd<1.2.0,>=1.1.0 in /root/miniconda3/envs/prp39/lib/python3.9/site-packages (from py7zr->rs-datasets) (1.1.0)\n",
      "Requirement already satisfied: pybcj<1.1.0,>=1.0.0 in /root/miniconda3/envs/prp39/lib/python3.9/site-packages (from py7zr->rs-datasets) (1.0.2)\n",
      "Requirement already satisfied: multivolumefile>=0.2.3 in /root/miniconda3/envs/prp39/lib/python3.9/site-packages (from py7zr->rs-datasets) (0.2.3)\n",
      "Requirement already satisfied: inflate64<1.1.0,>=1.0.0 in /root/miniconda3/envs/prp39/lib/python3.9/site-packages (from py7zr->rs-datasets) (1.0.0)\n",
      "Requirement already satisfied: brotli>=1.1.0 in /root/miniconda3/envs/prp39/lib/python3.9/site-packages (from py7zr->rs-datasets) (1.1.0)\n",
      "Requirement already satisfied: psutil in /root/miniconda3/envs/prp39/lib/python3.9/site-packages (from py7zr->rs-datasets) (6.0.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /root/miniconda3/envs/prp39/lib/python3.9/site-packages (from beautifulsoup4->gdown->rs-datasets) (2.5)\n",
      "Requirement already satisfied: webencodings in /root/miniconda3/envs/prp39/lib/python3.9/site-packages (from bleach->kaggle->rs-datasets) (0.5.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /root/miniconda3/envs/prp39/lib/python3.9/site-packages (from python-slugify->kaggle->rs-datasets) (1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/envs/prp39/lib/python3.9/site-packages (from requests[socks]->gdown->rs-datasets) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/envs/prp39/lib/python3.9/site-packages (from requests[socks]->gdown->rs-datasets) (3.7)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /root/miniconda3/envs/prp39/lib/python3.9/site-packages (from requests[socks]->gdown->rs-datasets) (1.7.1)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install rs-datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .time    { background: #40CC40; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tbody td { text-align: left; }\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .sp {  opacity: 0.25;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rs_datasets import MovieLens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "movielens = MovieLens(\"1m\")\n",
    "interactions = movielens.ratings\n",
    "user_features = movielens.users\n",
    "item_features = movielens.items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from replay.preprocessing.filters import NumInteractionsFilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interactions = NumInteractionsFilter(num_interactions=5).transform(interactions)\n",
    "sz = interactions.shape[0]\n",
    "cols = 8\n",
    "interactions[\"int_cat_list\"] = np.random.randint(0, 10, size=(sz, cols)).tolist()\n",
    "interactions[\"int_num_list\"] = np.random.randn(sz, cols).tolist()\n",
    "interactions[\"int_cat\"] = np.random.randint(0, 10, size=sz)\n",
    "interactions[\"int_num\"] = np.random.randn(sz)\n",
    "# interactions = pl.DataFrame(interactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>int_cat_list</th>\n",
       "      <th>int_num_list</th>\n",
       "      <th>int_cat</th>\n",
       "      <th>int_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "      <td>978300760</td>\n",
       "      <td>[4, 7, 5, 3, 9, 6, 6, 5]</td>\n",
       "      <td>[-0.27816860643294294, -0.9402160242022638, -0...</td>\n",
       "      <td>8</td>\n",
       "      <td>-0.943582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "      <td>978302109</td>\n",
       "      <td>[8, 7, 1, 3, 5, 9, 9, 9]</td>\n",
       "      <td>[0.32672808021581884, 0.14133843739609267, -1....</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.779098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "      <td>978301968</td>\n",
       "      <td>[5, 5, 9, 7, 9, 0, 3, 8]</td>\n",
       "      <td>[0.46296375108840315, -0.2595300691964234, 0.1...</td>\n",
       "      <td>9</td>\n",
       "      <td>0.601657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "      <td>978300275</td>\n",
       "      <td>[7, 7, 4, 7, 4, 0, 1, 4]</td>\n",
       "      <td>[1.8709501277109173, -0.6457708708502996, 1.31...</td>\n",
       "      <td>6</td>\n",
       "      <td>-2.331955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "      <td>978824291</td>\n",
       "      <td>[1, 4, 3, 3, 6, 9, 1, 3]</td>\n",
       "      <td>[0.3650871372795643, 0.04014247551687752, 0.34...</td>\n",
       "      <td>6</td>\n",
       "      <td>0.965065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000204</th>\n",
       "      <td>6040</td>\n",
       "      <td>1091</td>\n",
       "      <td>1</td>\n",
       "      <td>956716541</td>\n",
       "      <td>[7, 2, 6, 5, 7, 2, 8, 3]</td>\n",
       "      <td>[1.4377177589242183, -0.10815416627056858, -0....</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.758533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000205</th>\n",
       "      <td>6040</td>\n",
       "      <td>1094</td>\n",
       "      <td>5</td>\n",
       "      <td>956704887</td>\n",
       "      <td>[9, 8, 5, 6, 7, 0, 9, 2]</td>\n",
       "      <td>[1.2794104039537237, 0.9896057400696546, 1.897...</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.486710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000206</th>\n",
       "      <td>6040</td>\n",
       "      <td>562</td>\n",
       "      <td>5</td>\n",
       "      <td>956704746</td>\n",
       "      <td>[1, 6, 9, 3, 6, 5, 6, 5]</td>\n",
       "      <td>[-0.01008738778070476, 1.2067734905656247, 0.5...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.707191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000207</th>\n",
       "      <td>6040</td>\n",
       "      <td>1096</td>\n",
       "      <td>4</td>\n",
       "      <td>956715648</td>\n",
       "      <td>[6, 3, 6, 2, 1, 2, 6, 1]</td>\n",
       "      <td>[-0.8476870361541977, 1.491881104413442, 0.307...</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.604098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000208</th>\n",
       "      <td>6040</td>\n",
       "      <td>1097</td>\n",
       "      <td>4</td>\n",
       "      <td>956715569</td>\n",
       "      <td>[0, 1, 4, 3, 9, 2, 1, 2]</td>\n",
       "      <td>[-1.2618530991448405, -0.6023360195952016, 0.3...</td>\n",
       "      <td>6</td>\n",
       "      <td>0.722688</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000209 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         user_id  item_id  rating  timestamp              int_cat_list  \\\n",
       "0              1     1193       5  978300760  [4, 7, 5, 3, 9, 6, 6, 5]   \n",
       "1              1      661       3  978302109  [8, 7, 1, 3, 5, 9, 9, 9]   \n",
       "2              1      914       3  978301968  [5, 5, 9, 7, 9, 0, 3, 8]   \n",
       "3              1     3408       4  978300275  [7, 7, 4, 7, 4, 0, 1, 4]   \n",
       "4              1     2355       5  978824291  [1, 4, 3, 3, 6, 9, 1, 3]   \n",
       "...          ...      ...     ...        ...                       ...   \n",
       "1000204     6040     1091       1  956716541  [7, 2, 6, 5, 7, 2, 8, 3]   \n",
       "1000205     6040     1094       5  956704887  [9, 8, 5, 6, 7, 0, 9, 2]   \n",
       "1000206     6040      562       5  956704746  [1, 6, 9, 3, 6, 5, 6, 5]   \n",
       "1000207     6040     1096       4  956715648  [6, 3, 6, 2, 1, 2, 6, 1]   \n",
       "1000208     6040     1097       4  956715569  [0, 1, 4, 3, 9, 2, 1, 2]   \n",
       "\n",
       "                                              int_num_list  int_cat   int_num  \n",
       "0        [-0.27816860643294294, -0.9402160242022638, -0...        8 -0.943582  \n",
       "1        [0.32672808021581884, 0.14133843739609267, -1....        7 -0.779098  \n",
       "2        [0.46296375108840315, -0.2595300691964234, 0.1...        9  0.601657  \n",
       "3        [1.8709501277109173, -0.6457708708502996, 1.31...        6 -2.331955  \n",
       "4        [0.3650871372795643, 0.04014247551687752, 0.34...        6  0.965065  \n",
       "...                                                    ...      ...       ...  \n",
       "1000204  [1.4377177589242183, -0.10815416627056858, -0....        6 -0.758533  \n",
       "1000205  [1.2794104039537237, 0.9896057400696546, 1.897...        1 -1.486710  \n",
       "1000206  [-0.01008738778070476, 1.2067734905656247, 0.5...        0  0.707191  \n",
       "1000207  [-0.8476870361541977, 1.491881104413442, 0.307...        6 -0.604098  \n",
       "1000208  [-1.2618530991448405, -0.6023360195952016, 0.3...        6  0.722688  \n",
       "\n",
       "[1000209 rows x 8 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'int_cat_list': {4: 0, 7: 1, 5: 2, 3: 3, 9: 4, 6: 5, 8: 6, 1: 7, 0: 8, 2: 9}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from replay.preprocessing import GroupedLabelEncodingRule, LabelEncoder\n",
    "\n",
    "le = LabelEncoder([GroupedLabelEncodingRule(\"int_cat_list\")])\n",
    "le.fit(interactions)\n",
    "le.mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = le.transform(interactions)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz = user_features.shape[0]\n",
    "cols = 6\n",
    "user_features[\"user_cat_list\"] = np.random.randint(0, 10, size=(sz, cols)).tolist()\n",
    "user_features[\"user_num_list\"] = np.random.randn(sz, cols).tolist()\n",
    "user_features[\"user_cat\"] = np.random.randint(0, 10, size=sz)\n",
    "user_features[\"user_num\"] = np.random.randn(sz)\n",
    "# user_features = pl.DataFrame(user_features)\n",
    "user_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz = item_features.shape[0]\n",
    "cols = 13\n",
    "item_features.sort_values(\"item_id\", inplace=True)\n",
    "item_features[\"item_cat_list\"] = np.random.randint(0, 10, size=(sz, cols)).tolist()\n",
    "item_features[\"item_num_list\"] = np.random.randn(sz, cols).tolist()\n",
    "item_features[\"item_cat\"] = np.random.randint(0, 10, size=sz).tolist()\n",
    "item_features[\"item_num\"] = np.random.randn(sz).tolist()\n",
    "# item_features = pl.DataFrame(item_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing duplicates in the timestamp column without changing the original items order where timestamp is the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interactions[\"timestamp\"] = interactions[\"timestamp\"].astype(\"int64\")\n",
    "# interactions = interactions.sort_values(by=\"timestamp\")\n",
    "# interactions[\"timestamp\"] = interactions.groupby(\"user_id\").cumcount()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split interactions into the train, validation and test datasets using LastNSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = LastNSplitter(\n",
    "    N=1,\n",
    "    divide_column=\"user_id\",\n",
    "    query_column=\"user_id\",\n",
    "    strategy=\"interactions\",\n",
    ")\n",
    "\n",
    "raw_test_events, raw_test_gt = splitter.split(interactions)\n",
    "raw_validation_events, raw_validation_gt = splitter.split(raw_test_events)\n",
    "raw_train_events = raw_validation_events"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare FeatureSchema required to create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_feature_schema(is_ground_truth: bool) -> FeatureSchema:\n",
    "    all_features = FeatureSchema(\n",
    "        [\n",
    "            FeatureInfo(\n",
    "                column=\"user_id\",\n",
    "                feature_hint=FeatureHint.QUERY_ID,\n",
    "                feature_type=FeatureType.CATEGORICAL,\n",
    "            ),\n",
    "            FeatureInfo(\n",
    "                column=\"item_id\",\n",
    "                feature_hint=FeatureHint.ITEM_ID,\n",
    "                feature_type=FeatureType.CATEGORICAL,\n",
    "            ),\n",
    "            FeatureInfo(\n",
    "                column=\"int_cat_list\",\n",
    "                feature_type=FeatureType.CATEGORICAL_LIST,\n",
    "            ),\n",
    "            FeatureInfo(\n",
    "                column=\"int_num_list\",\n",
    "                feature_type=FeatureType.NUMERICAL_LIST,\n",
    "            ),\n",
    "            FeatureInfo(\n",
    "                column=\"int_cat\",\n",
    "                feature_type=FeatureType.CATEGORICAL,\n",
    "            ),\n",
    "            FeatureInfo(\n",
    "                column=\"int_num\",\n",
    "                feature_type=FeatureType.NUMERICAL,\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    if is_ground_truth:\n",
    "        return all_features\n",
    "\n",
    "    all_features = all_features + FeatureSchema(\n",
    "        [\n",
    "            FeatureInfo(\n",
    "                column=\"timestamp\",\n",
    "                feature_type=FeatureType.NUMERICAL,\n",
    "                feature_hint=FeatureHint.TIMESTAMP,\n",
    "            ),\n",
    "            # item features\n",
    "            FeatureInfo(\n",
    "                column=\"item_cat_list\",\n",
    "                feature_type=FeatureType.CATEGORICAL_LIST,\n",
    "                feature_source=FeatureSource.ITEM_FEATURES,\n",
    "            ),\n",
    "            FeatureInfo(\n",
    "                column=\"item_num_list\",\n",
    "                feature_type=FeatureType.NUMERICAL_LIST,\n",
    "                feature_source=FeatureSource.ITEM_FEATURES,\n",
    "            ),\n",
    "            FeatureInfo(\n",
    "                column=\"item_cat\",\n",
    "                feature_type=FeatureType.CATEGORICAL,\n",
    "                feature_source=FeatureSource.ITEM_FEATURES,\n",
    "            ),\n",
    "            FeatureInfo(\n",
    "                column=\"item_num\",\n",
    "                feature_type=FeatureType.NUMERICAL,\n",
    "                feature_source=FeatureSource.ITEM_FEATURES,\n",
    "            ),\n",
    "            # query features\n",
    "            FeatureInfo(\n",
    "                column=\"user_cat_list\",\n",
    "                feature_type=FeatureType.CATEGORICAL_LIST,\n",
    "                feature_source=FeatureSource.QUERY_FEATURES,\n",
    "            ),\n",
    "            FeatureInfo(\n",
    "                column=\"user_num_list\",\n",
    "                feature_type=FeatureType.NUMERICAL_LIST,\n",
    "                feature_source=FeatureSource.QUERY_FEATURES,\n",
    "            ),\n",
    "            FeatureInfo(\n",
    "                column=\"user_cat\",\n",
    "                feature_type=FeatureType.CATEGORICAL,\n",
    "                feature_source=FeatureSource.QUERY_FEATURES,\n",
    "            ),\n",
    "            FeatureInfo(\n",
    "                column=\"user_num\",\n",
    "                feature_type=FeatureType.NUMERICAL,\n",
    "                feature_source=FeatureSource.QUERY_FEATURES,\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    return all_features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Dataset for the training stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(\n",
    "    feature_schema=prepare_feature_schema(is_ground_truth=False),\n",
    "    interactions=raw_train_events,\n",
    "    query_features=user_features,\n",
    "    item_features=item_features,\n",
    "    check_consistency=True,\n",
    "    categorical_encoded=False,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Datasets (events and ground_truth) for the validation stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataset = Dataset(\n",
    "    feature_schema=prepare_feature_schema(is_ground_truth=False),\n",
    "    interactions=raw_validation_events,\n",
    "    query_features=user_features,\n",
    "    item_features=item_features,\n",
    "    check_consistency=True,\n",
    "    categorical_encoded=False,\n",
    ")\n",
    "validation_gt = Dataset(\n",
    "    feature_schema=prepare_feature_schema(is_ground_truth=True),\n",
    "    interactions=raw_validation_gt,\n",
    "    check_consistency=True,\n",
    "    categorical_encoded=False,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Datasets (events and ground_truth) for the testing stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = Dataset(\n",
    "    feature_schema=prepare_feature_schema(is_ground_truth=False),\n",
    "    interactions=raw_test_events,\n",
    "    query_features=user_features,\n",
    "    item_features=item_features,\n",
    "    check_consistency=True,\n",
    "    categorical_encoded=False,\n",
    ")\n",
    "test_gt = Dataset(\n",
    "    feature_schema=prepare_feature_schema(is_ground_truth=True),\n",
    "    interactions=raw_test_gt,\n",
    "    check_consistency=True,\n",
    "    categorical_encoded=False,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the tensor schema\n",
    "A schema shows the correspondence of columns from the source dataset with the internal representation of tensors inside the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ITEM_FEATURE_NAME = \"item_id_seq\"\n",
    "\n",
    "tensor_schema = TensorSchema([\n",
    "    TensorFeatureInfo(\n",
    "        name=ITEM_FEATURE_NAME,\n",
    "        is_seq=True,\n",
    "        feature_type=FeatureType.CATEGORICAL,\n",
    "        feature_source=TensorFeatureSource(FeatureSource.INTERACTIONS, train_dataset.feature_schema.item_id_column),\n",
    "        feature_hint=FeatureHint.ITEM_ID,\n",
    "    ),\n",
    "    TensorFeatureInfo(\n",
    "        name=\"timestamp_seq\",\n",
    "        is_seq=True,\n",
    "        feature_type=FeatureType.NUMERICAL,\n",
    "        feature_source=TensorFeatureSource(FeatureSource.INTERACTIONS, \"timestamp\"),\n",
    "        feature_hint=FeatureHint.TIMESTAMP,\n",
    "    ),\n",
    "    TensorFeatureInfo(\n",
    "        name=\"item_cat_list_seq\",\n",
    "        is_seq=True,\n",
    "        feature_type=FeatureType.CATEGORICAL_LIST,\n",
    "        feature_source=TensorFeatureSource(FeatureSource.ITEM_FEATURES, \"item_cat_list\"),\n",
    "    ),\n",
    "    TensorFeatureInfo(\n",
    "        name=\"item_num_list_seq\",\n",
    "        is_seq=True,\n",
    "        feature_type=FeatureType.NUMERICAL_LIST,\n",
    "        feature_source=TensorFeatureSource(FeatureSource.ITEM_FEATURES, \"item_num_list\"),\n",
    "    ),\n",
    "    TensorFeatureInfo(\n",
    "        name=\"item_cat_seq\",\n",
    "        is_seq=True,\n",
    "        feature_type=FeatureType.CATEGORICAL,\n",
    "        feature_source=TensorFeatureSource(FeatureSource.ITEM_FEATURES, \"item_cat\"),\n",
    "    ),\n",
    "    TensorFeatureInfo(\n",
    "        name=\"item_num_seq\",\n",
    "        is_seq=True,\n",
    "        feature_type=FeatureType.NUMERICAL,\n",
    "        feature_source=TensorFeatureSource(FeatureSource.ITEM_FEATURES, \"item_num\"),\n",
    "    ),\n",
    "    TensorFeatureInfo(\n",
    "        name=\"int_cat_list_seq\",\n",
    "        is_seq=True,\n",
    "        feature_type=FeatureType.CATEGORICAL_LIST,\n",
    "        feature_source=TensorFeatureSource(FeatureSource.INTERACTIONS, \"int_cat_list\"),\n",
    "    ),\n",
    "    TensorFeatureInfo(\n",
    "        name=\"int_num_list_seq\",\n",
    "        is_seq=True,\n",
    "        feature_type=FeatureType.NUMERICAL_LIST,\n",
    "        feature_source=TensorFeatureSource(FeatureSource.INTERACTIONS, \"int_num_list\"),\n",
    "    ),\n",
    "    TensorFeatureInfo(\n",
    "        name=\"int_cat_seq\",\n",
    "        is_seq=True,\n",
    "        feature_type=FeatureType.CATEGORICAL,\n",
    "        feature_source=TensorFeatureSource(FeatureSource.INTERACTIONS, \"int_cat\"),\n",
    "    ),\n",
    "    TensorFeatureInfo(\n",
    "        name=\"int_num_seq\",\n",
    "        is_seq=True,\n",
    "        feature_type=FeatureType.NUMERICAL,\n",
    "        feature_source=TensorFeatureSource(FeatureSource.INTERACTIONS, \"int_num\"),\n",
    "    ),\n",
    "    TensorFeatureInfo(\n",
    "        name=\"user_cat_list_seq\",\n",
    "        is_seq=True,\n",
    "        feature_type=FeatureType.CATEGORICAL_LIST,\n",
    "        feature_source=TensorFeatureSource(FeatureSource.QUERY_FEATURES, \"user_cat_list\"),\n",
    "    ),\n",
    "    TensorFeatureInfo(\n",
    "        name=\"user_cat_list_not_seq\",\n",
    "        is_seq=False,\n",
    "        feature_type=FeatureType.CATEGORICAL_LIST,\n",
    "        feature_source=TensorFeatureSource(FeatureSource.QUERY_FEATURES, \"user_cat_list\"),\n",
    "    ),\n",
    "    TensorFeatureInfo(\n",
    "        name=\"user_cat_seq\",\n",
    "        is_seq=True,\n",
    "        feature_type=FeatureType.CATEGORICAL,\n",
    "        feature_source=TensorFeatureSource(FeatureSource.QUERY_FEATURES, \"user_cat\"),\n",
    "    ),\n",
    "    TensorFeatureInfo(\n",
    "        name=\"user_cat_not_seq\",\n",
    "        is_seq=False,\n",
    "        feature_type=FeatureType.CATEGORICAL,\n",
    "        feature_source=TensorFeatureSource(FeatureSource.QUERY_FEATURES, \"user_cat\"),\n",
    "    ),\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create sequential datasets using SequenceTokenizer\n",
    "The SequentialDataset internally store data in the form of sequences of items sorted by increasing interaction time (timestamp). A SequenceTokenizer is used to convert to this format. In addition, the SequenceTokenizer encodes all categorical columns from the source dataset and stores mapping inside itself.\n",
    "SequentialDataset.keep_common_query_ids is used to leave only sequences from the same users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = SequenceTokenizer(tensor_schema, allow_collect_to_master=True)\n",
    "tokenizer.fit(train_dataset)\n",
    "\n",
    "sequential_train_dataset = tokenizer.transform(train_dataset)\n",
    "\n",
    "sequential_validation_dataset = tokenizer.transform(validation_dataset)\n",
    "sequential_validation_gt = tokenizer.transform(validation_gt, [tensor_schema.item_id_feature_name])\n",
    "\n",
    "sequential_validation_dataset, sequential_validation_gt = SequentialDataset.keep_common_query_ids(\n",
    "    sequential_validation_dataset, sequential_validation_gt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequential_train_dataset._sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = sequential_train_dataset.get_sequence(0, \"item_id_seq\")\n",
    "print(seq.shape)\n",
    "seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = sequential_train_dataset.get_sequence(0, \"user_cat_not_seq\")\n",
    "print(seq.shape)\n",
    "seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query_ids = test_gt.query_ids\n",
    "test_query_ids_np = tokenizer.query_id_encoder.transform(test_query_ids)[\"user_id\"].to_numpy()\n",
    "sequential_test_dataset = tokenizer.transform(test_dataset).filter_by_query_id(test_query_ids_np)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get the user and item mapping and inverse mapping as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.query_id_encoder.mapping, tokenizer.query_id_encoder.inverse_mapping)\n",
    "print(tokenizer.item_id_encoder.mapping, tokenizer.item_id_encoder.inverse_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    dataset=SasRecTrainingDataset(\n",
    "        sequential_train_dataset,\n",
    "        max_sequence_length=10,\n",
    "    ),\n",
    "    batch_size=3,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dataloader:\n",
    "    print(batch.features[\"item_id_seq\"])\n",
    "\n",
    "    print(batch.query_id)\n",
    "    print(batch.features[\"user_cat_not_seq\"])\n",
    "    print(batch.features[\"user_cat_list_not_seq\"])\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model\n",
    "### Create SASRec model instance and run the training stage using lightning\n",
    "After each epoch validation metrics are shown. You can change the list of validation metrics in ValidationMetricsCallback\n",
    "The model is determined to be the best and is saved if the metric updates its maximum during validation (see the ModelCheckpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LEN = 200\n",
    "BATCH_SIZE = 512\n",
    "NUM_WORKERS = 4\n",
    "\n",
    "model = SasRec(\n",
    "    tensor_schema,\n",
    "    block_count=2,\n",
    "    head_count=2,\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    "    hidden_size=300,\n",
    "    dropout_rate=0.5,\n",
    "    optimizer_factory=FatOptimizerFactory(learning_rate=0.001),\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(save_dir=\".logs/train\", name=\"SASRec_example\")\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\".checkpoints\",\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    # if you use multiple dataloaders, then add the serial number of the dataloader to the suffix of the metric name.\n",
    "    # For example,\"recall@10/dataloader_idx_0\"\n",
    "    monitor=\"recall@10\",\n",
    "    mode=\"max\",\n",
    ")\n",
    "\n",
    "validation_metrics_callback = ValidationMetricsCallback(\n",
    "    metrics=[\"map\", \"ndcg\", \"recall\"],\n",
    "    ks=[1, 5, 10, 20],\n",
    "    item_count=train_dataset.item_count,\n",
    "    postprocessors=[RemoveSeenItems(sequential_validation_dataset)]\n",
    ")\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=100,\n",
    "    callbacks=[checkpoint_callback, validation_metrics_callback],\n",
    "    logger=csv_logger,\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=SasRecTrainingDataset(\n",
    "        sequential_train_dataset,\n",
    "        max_sequence_length=MAX_SEQ_LEN,\n",
    "    ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=4,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "    dataset=SasRecValidationDataset(\n",
    "        sequential_validation_dataset,\n",
    "        sequential_validation_gt,\n",
    "        sequential_train_dataset,\n",
    "        max_sequence_length=MAX_SEQ_LEN,\n",
    "    ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    model,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=validation_dataloader,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The path to the best model is saved inside checkpoint_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = SasRec.load_from_checkpoint(checkpoint_callback.best_model_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference stage\n",
    "### Prepare Dataloader and logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_dataloader = DataLoader(\n",
    "    dataset=SasRecPredictionDataset(\n",
    "        sequential_test_dataset,\n",
    "        max_sequence_length=MAX_SEQ_LEN,\n",
    "    ),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "csv_logger = CSVLogger(save_dir=\".logs/test\", name=\"SASRec_example\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run inference\n",
    "You can get the recommendations in three formats: PySpark DataFrame, Pandas DataFrame, PyTorch tensors. Each of the types corresponds a callback\n",
    "You can filter the results using postprocessors strategy. For example the RemoveSeenItems postprocessor is filtering out the items that already have been seen in test dataset\n",
    "You don't need to use all three callbacks. This is shown only for example\n",
    "\n",
    "Also, you can get user embeddings, that were used to perform predictions, using `get_query_embedding` method inside SasRecModel or `QueryEmbeddingsPredictionCallback` for lightning module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPK = [1, 10, 20, 100]\n",
    "\n",
    "postprocessors = [RemoveSeenItems(sequential_test_dataset)]\n",
    "\n",
    "spark_prediction_callback = SparkPredictionCallback(\n",
    "    spark_session=spark_session,\n",
    "    top_k=max(TOPK),\n",
    "    query_column=\"user_id\",\n",
    "    item_column=\"item_id\",\n",
    "    rating_column=\"score\",\n",
    "    postprocessors=postprocessors,\n",
    ")\n",
    "\n",
    "pandas_prediction_callback = PandasPredictionCallback(\n",
    "    top_k=max(TOPK),\n",
    "    query_column=\"user_id\",\n",
    "    item_column=\"item_id\",\n",
    "    rating_column=\"score\",\n",
    "    postprocessors=postprocessors,\n",
    ")\n",
    "\n",
    "torch_prediction_callback = TorchPredictionCallback(\n",
    "    top_k=max(TOPK),\n",
    "    postprocessors=postprocessors,\n",
    ")\n",
    "\n",
    "query_embeddings_callback = QueryEmbeddingsPredictionCallback()\n",
    "\n",
    "trainer = L.Trainer(\n",
    "    callbacks=[\n",
    "        spark_prediction_callback,\n",
    "        pandas_prediction_callback,\n",
    "        torch_prediction_callback,\n",
    "        query_embeddings_callback\n",
    "    ], \n",
    "    logger=csv_logger, \n",
    "    inference_mode=True\n",
    ")\n",
    "trainer.predict(best_model, dataloaders=prediction_dataloader, return_predictions=False)\n",
    "\n",
    "spark_res = spark_prediction_callback.get_result()\n",
    "pandas_res = pandas_prediction_callback.get_result()\n",
    "torch_user_ids, torch_item_ids, torch_scores = torch_prediction_callback.get_result()\n",
    "user_embeddings = query_embeddings_callback.get_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_res.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch_user_ids[0], torch_item_ids[0], torch_scores[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we want to get the recomendations in PySpark format. \n",
    "Let's get the inverse representation of labels using inverse_transform method.\n",
    "\n",
    "Note that the reverse representation can only be obtained for PySpark and Pandas formats. When working with PyTorch tensors, the reverse representation must be done manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = tokenizer.query_and_item_id_encoder.inverse_transform(spark_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_args = {\"query_column\": \"user_id\", \"rating_column\": \"score\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_metrics = OfflineMetrics(\n",
    "    [Recall(TOPK), Precision(TOPK), MAP(TOPK), NDCG(TOPK), MRR(TOPK), HitRate(TOPK)], **init_args\n",
    ")(recommendations.toPandas(), raw_test_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_to_df(result_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Got 6040 x 300 user embeddings, because among all 12 batches: \n",
    "\n",
    "11 batches contains 512 samples\n",
    "\n",
    "1 batch contains 408 left samples\n",
    "\n",
    "11 * 512 + 408 == 6040"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access user embeddings directly with `SasRecModel` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "core_model = SasRecModel(\n",
    "    tensor_schema,\n",
    "    num_blocks=2,\n",
    "    num_heads=2,\n",
    "    max_len=MAX_SEQ_LEN,\n",
    "    hidden_size=300,\n",
    "    dropout=0.5\n",
    ")\n",
    "core_model.eval()\n",
    "core_model = core_model.to(device)\n",
    "\n",
    "# Get first batch of data \n",
    "data = next(iter(prediction_dataloader))\n",
    "tensor_map, padding_mask = data.features, data.padding_mask\n",
    "\n",
    "# Ensure everything is on the same device\n",
    "padding_mask = padding_mask.to(device)\n",
    "tensor_map[\"item_id_seq\"] = tensor_map[\"item_id_seq\"].to(device)\n",
    "\n",
    "# Get user embeddings\n",
    "user_embeddings_batch = core_model.get_query_embeddings(tensor_map, padding_mask)\n",
    "user_embeddings_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_embeddings_batch.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_all_embeddings()` method in transformers can be used to get copies of all embeddings that are presented in model as a dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings = best_model.get_all_embeddings()\n",
    "all_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can access item embeddings from this dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_embeddings = all_embeddings[\"item_embedding\"]\n",
    "item_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Item embeddings shape is (N_ITEMS, HIDDEN_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure we got correct dimension and ensure we got the copy of tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert item_embeddings.shape[0] == len(tokenizer.item_id_encoder.mapping[\"item_id\"])\n",
    "assert id(item_embeddings) != id(best_model._model.item_embedder.item_emb.weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example we observe one new item id in our training data. We can easily expand our item embedder by one element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to expand item embeddings by new size `set_item_embeddings_by_size` method is applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.set_item_embeddings_by_size(item_embeddings.shape[0] + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our new item embeddings have one extra embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_size = best_model.get_all_embeddings()[\"item_embedding\"].shape[0]\n",
    "old_size = item_embeddings.shape[0]\n",
    "\n",
    "assert new_size == old_size + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can pass our item embeddings that replace the existing ones by calling `set_item_embeddings_by_tensor`.\n",
    "\n",
    "If tensor contains new items, they will be added to item embedder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_embeddings_weights = torch.rand((new_size + 1, 300))    # randint used for example only\n",
    "\n",
    "best_model.set_item_embeddings_by_tensor(new_embeddings_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the moment we expanded our item embeddings by one more item and replace weights by passing `new_embeddings_weights`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_size = new_size\n",
    "new_size = best_model.get_all_embeddings()[\"item_embedding\"].shape[0]\n",
    "\n",
    "assert new_size == old_size + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can append tensor for only new items with no replace for existing by calling `append_item_embeddings`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_item_weights = torch.rand((1, 300))    # randint used for example only\n",
    "\n",
    "best_model.append_item_embeddings(new_item_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We passed one new example and its weights to item embeddings, thus expanded our vocabulary by one item again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_size = new_size\n",
    "new_size = best_model.get_all_embeddings()[\"item_embedding\"].shape[0]\n",
    "\n",
    "assert new_size == old_size + 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of launching an inference for a single user without using a trainer (in order to speed up)\n",
    "An example for the production of an online script\n",
    "\n",
    "Let's assume that the user's sequence consisted of a sequence of items [1, 2, 3, 4, 5]. \n",
    "Сreate a padding mask corresponding to the sequence of items.\n",
    "\n",
    "It is important to take only the latest MAX_SEQ_LEN or less items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_sequence = torch.arange(1, 5).unsqueeze(0)[:, -MAX_SEQ_LEN:]\n",
    "padding_mask = torch.ones_like(item_sequence, dtype=torch.bool)\n",
    "sequence_item_count = item_sequence.shape[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrapping created tensors in the SasRecPredictionBatch entity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = SasRecPredictionBatch(\n",
    "    query_id=torch.arange(0, item_sequence.shape[0], 1).long(),\n",
    "    padding_mask=padding_mask.bool(),\n",
    "    features={ITEM_FEATURE_NAME: item_sequence.long()}\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run predict step of the SasRec and get scores from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    scores = best_model.predict_step(batch, 0)\n",
    "scores"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting three items with the highest score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.topk(scores, k=3).indices"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0857f111b041889635bea848a6a183706c3f1c18c9dafdb447caa5e8bea01452"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
